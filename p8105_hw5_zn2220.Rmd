---
title: "p8105_hw5_zn2220"
author: "Ziang Niu"
date: "2025-10-31"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)

set.seed(235)
```

# Problem 1

## Write a function that, for a fixed group size, randomly draws “birthdays” for each person; checks whether there are duplicate birthdays in the group; and returns TRUE or FALSE based on the result.

Write the function.

```{r}
duplicate_birthday <- function(n) {
birthdays <- sample(1:365, size = n, replace = TRUE)
any(duplicated(birthdays))
}
```

Check returns.

```{r}
duplicate_birthday(2)
duplicate_birthday(23)
duplicate_birthday(50)
```

## Next, run this function 10000 times for each group size between 2 and 50. For each group size, compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs. Make a plot showing the probability as a function of group size, and comment on your results.

Create a tibble.

```{r}
n <- 2:50
repeats <- 10000

prob <- numeric(length(n))

for (i in seq_along(n)) {
  group_size <- n[i]
  prob[i] <- mean(replicate(repeats, duplicate_birthday(group_size)))
}

prob_list <- tibble(n = n, probability = prob)

```

draw a plot.

```{r}
ggplot(prob_list, aes(x = n, y = prob)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = seq(0, 50, by = 5)) +
labs(title = "Probability that at least two people share a birthday",
x = "Group size (n)",
y = "Probability") +
theme_minimal(base_size = 12)
```

As n increases, the probability of shared birthdays rises quickly; at around n = 23, it’s already about 0.5.

# Problem 2

### Set up

```{r}
run_t_sim <- function(mu, n = 30, sigma = 5, repeats = 5000, alpha = 0.05) {
  tibble(rep = 1:repeats) %>%
    mutate(
      data = map(rep, ~ rnorm(n, mean = mu, sd = sigma)),
      test = map(data, ~ broom::tidy(t.test(.x, mu = 0))),
      mu_hat = map_dbl(test, "estimate"),
      p_value = map_dbl(test, "p.value"),
      reject = p_value < alpha
    )
}

mus <- 0:6

results_p2 <- data.frame(
  mu = numeric(),
  power = numeric(),
  mean_mu_hat_all = numeric(),
  mean_mu_hat_reject = numeric()
)

for (m in mus) {
  df <- run_t_sim(mu = m)
  
  power <- mean(df$reject)
  mean_mu_hat_all <- mean(df$mu_hat)
  mean_mu_hat_reject <- if (any(df$reject)) mean(df$mu_hat[df$reject]) else NA
  
  results_p2 <- rbind(
    results_p2,
    data.frame(
      mu = m,
      power = power,
      mean_mu_hat_all = mean_mu_hat_all,
      mean_mu_hat_reject = mean_mu_hat_reject
    )
  )
}

results_p2
```

### Plot 1

```{r}
ggplot(results_p2, aes(x = mu, y = power)) +
  geom_line(size = 1) + 
  geom_point(size = 2) +
  labs(
    title = "Power of One-Sample t-Test vs Effect Size",
    x = expression("True Mean " ~ mu),
    y = "Power (Proportion Rejected)"
  ) +
  theme_minimal(base_size = 12)
```

The plot shows a strong, positive, non-linear relationship between true mean (μ) and test power: when μ=0, power ≈0.05 (the Type I error rate), and as μ increases, power rises quickly—exceeding 0.5 by μ=2 and approaching 1.0 by μ=4, indicating that larger effect sizes yield higher statistical power.

### Plot 2

```{r}
results_p2_long <- results_p2 %>%
  pivot_longer(
    cols = c(mean_mu_hat_all, mean_mu_hat_reject),
    names_to = "sample_type",
    values_to = "mean_mu_hat"
  )

ggplot(results_p2_long, aes(x = mu, y = mean_mu_hat, color = sample_type)) +
  geom_line(size = 1) + 
  geom_point(size = 2) +
  labs(
    title = expression("Average " * hat(mu) * " Across All vs Rejected Samples"),
    x = expression("True Mean " ~ mu),
    y = expression("Average Estimate " * hat(mu)),
    color = "Sample Type"
  ) +
  scale_color_manual(
    labels = c("All Samples", "Only Rejected H0"),
    values = c("navy", "red")
  ) +
  theme_minimal(base_size = 12)

```


No, the average μ̂ from only the rejected samples (red line) is not equal to the true μ, especially for small effect sizes. The blue line (all samples) is unbiased and follows y = x, but the red line is biased upward when power is low—for instance, when μ = 1, the average μ̂ exceeds 2. This occurs due to selection bias or the “winner’s curse”: only simulations with unusually large random deviations achieve significance (p < 0.05), inflating the estimated effect. As μ increases and power approaches 1, nearly all tests are significant, eliminating this bias and causing the red and blue lines to converge.

# Problem 3

## Describe the raw data. Create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r}
data_url <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicides_raw <- read_csv(data_url)

summary(homicides_raw)

city_summary <- homicides_raw %>%
  mutate(city_state = str_c(city, state, sep = ", ")) %>%
  group_by(city_state) %>%
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  ) %>%
  ungroup()
```

## For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
baltimore_data <- city_summary %>%
  filter(city_state == "Baltimore, MD")

baltimore_prop_test <- prop.test(
  x = baltimore_data$unsolved_homicides,
  n = baltimore_data$total_homicides
)

baltimore_tidy <- broom::tidy(baltimore_prop_test)

baltimore_tidy
```

## Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r}
city_proportions_tidy <- city_summary %>%
  mutate(
    prop_test_obj = map2(unsolved_homicides, total_homicides, 
                         ~ prop.test(x = .x, n = .y))
  ) %>%
  mutate(
    tidy_results = map(prop_test_obj, broom::tidy)
  ) %>%
  unnest(tidy_results) %>%
  select(
    city_state, 
    total_homicides, 
    unsolved_homicides, 
    estimate, 
    conf.low, 
    conf.high
  )

summary(city_proportions_tidy)
```

## Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r}
ggplot(city_proportions_tidy, 
       aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point(color = "darkblue") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by U.S. City",
    subtitle = "Estimates and 95% Confidence Intervals",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  theme_minimal(base_size = 12) +
  scale_y_continuous(labels = scales::percent_format())
```